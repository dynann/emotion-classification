{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d014d2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Devv\\Machine Learning\\huggingface\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2Processor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43b7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset_name\": \"stapesai/ssi-speech-emotion-recognition\",\n",
    "    \"output_dir\": \"./multimodal-emotion-recognition\",\n",
    "    \"sample_rate\": 16000,\n",
    "    \"max_duration\": 6.0,\n",
    "    \"batch_size\": 4, # Reduced batch size largely due to duplicate models in VRAM\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"freeze_feature_encoder\": True,\n",
    "    \"text_model_name\": \"roberta-base\",\n",
    "    \"audio_model_name\": \"facebook/wav2vec2-base\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac047ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3967a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380e27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, dataset, label2id, audio_processor, text_tokenizer, augment=False):\n",
    "        self.dataset = dataset\n",
    "        self.label2id = label2id\n",
    "        self.audio_processor = audio_processor\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.augment = augment\n",
    "        self.max_samples = int(CONFIG[\"sample_rate\"] * CONFIG[\"max_duration\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def augment_audio(self, x):\n",
    "        if np.random.rand() < 0.5:\n",
    "            x += 0.005 * np.random.randn(len(x))\n",
    "        if np.random.rand() < 0.5:\n",
    "            x *= np.random.uniform(0.7, 1.3)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # --- Audio Processing ---\n",
    "        # The 'file_path' column is the Audio column in this dataset\n",
    "        audio_array = item[\"file_path\"][\"array\"]\n",
    "            \n",
    "        # Ensure correct length / truncation\n",
    "        if len(audio_array) > self.max_samples:\n",
    "            audio_array = audio_array[:self.max_samples]\n",
    "            \n",
    "        if self.augment:\n",
    "            audio_array = self.augment_audio(audio_array)\n",
    "\n",
    "        audio_inputs = self.audio_processor(\n",
    "            audio_array,\n",
    "            sampling_rate=CONFIG[\"sample_rate\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        audio_input_values = audio_inputs.input_values.squeeze(0)\n",
    "        \n",
    "        # Create audio mask (1 for valid, 0 for padding - though here we haven't padded yet, collator typically handles it)\n",
    "        if hasattr(audio_inputs, \"attention_mask\") and audio_inputs.attention_mask is not None:\n",
    "             audio_attention_mask = audio_inputs.attention_mask.squeeze(0)\n",
    "        else:\n",
    "             audio_attention_mask = torch.ones(audio_input_values.shape[0], dtype=torch.long)\n",
    "\n",
    "        # --- Text Processing ---\n",
    "        text = item[\"text\"]\n",
    "        if text is None:\n",
    "            text = \"\" # Handle missing text if any\n",
    "            \n",
    "        text_inputs = self.text_tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = text_inputs.input_ids.squeeze(0)\n",
    "        text_attention_mask = text_inputs.attention_mask.squeeze(0)\n",
    "\n",
    "        label_str = item[\"emotion\"]\n",
    "\n",
    "        return {\n",
    "            \"input_values\": audio_input_values,\n",
    "            \"audio_attention_mask\": audio_attention_mask,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": text_attention_mask, # Standard name for transformers (text)\n",
    "            \"labels\": torch.tensor(self.label2id[label_str], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7bb21b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Devv\\Machine Learning\\huggingface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Devv\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91093992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmotionModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        # Audio Encoder\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(CONFIG[\"audio_model_name\"])\n",
    "        if hasattr(self.wav2vec, \"gradient_checkpointing_disable\"):\n",
    "            self.wav2vec.gradient_checkpointing_disable()\n",
    "            \n",
    "        # Text Encoder\n",
    "        self.roberta = RobertaModel.from_pretrained(CONFIG[\"text_model_name\"])\n",
    "        \n",
    "        # Classifier\n",
    "        # Audio dim (768) + Text dim (768) = 1536\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec.config.hidden_size + self.roberta.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_labels),\n",
    "        )\n",
    "\n",
    "    def freeze_feature_encoder(self):\n",
    "        self.wav2vec.feature_extractor._freeze_parameters()\n",
    "        # Optionally freeze roberta embeddings if needed\n",
    "        # for param in self.roberta.embeddings.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_values, audio_attention_mask=None, input_ids=None, attention_mask=None, labels=None):\n",
    "        # --- Audio Forward ---\n",
    "        # Wav2Vec2 expects 'attention_mask' argument for its mask. We pass audio_attention_mask to it.\n",
    "        audio_outputs = self.wav2vec(input_values, attention_mask=audio_attention_mask)\n",
    "        audio_hidden = audio_outputs.last_hidden_state\n",
    "\n",
    "        # Mean Pooling for Audio\n",
    "        if audio_attention_mask is not None:\n",
    "            # Wav2Vec2 downsamples time, so convert sample-level mask to feature-level mask\n",
    "            if hasattr(self.wav2vec, \"_get_feature_vector_attention_mask\"):\n",
    "                feat_mask = self.wav2vec._get_feature_vector_attention_mask(\n",
    "                    audio_hidden.shape[1], audio_attention_mask\n",
    "                )\n",
    "            else:\n",
    "                # Fallback if specific method not available\n",
    "                scale = audio_hidden.shape[1] / audio_attention_mask.shape[1]\n",
    "                feat_mask_len = (audio_attention_mask.sum(dim=1) * scale).long()\n",
    "                feat_mask = torch.zeros(audio_hidden.shape[:2], device=audio_hidden.device)\n",
    "                for i, l in enumerate(feat_mask_len):\n",
    "                    feat_mask[i, :l] = 1\n",
    "            \n",
    "            mask = feat_mask.unsqueeze(-1).to(dtype=audio_hidden.dtype)\n",
    "            denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "            audio_pooled = (audio_hidden * mask).sum(dim=1) / denom\n",
    "        else:\n",
    "            audio_pooled = torch.mean(audio_hidden, dim=1)\n",
    "\n",
    "        # --- Text Forward ---\n",
    "        text_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use CLS token representation (first token)\n",
    "        text_pooled = text_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # --- Concatenate ---\n",
    "        # Shape: (Batch, 768 + 768)\n",
    "        combined_features = torch.cat((audio_pooled, text_pooled), dim=1)\n",
    "\n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(combined_features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867d2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b614aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    # Manually collate to handle padding for audio inputs_values which might differ in length\n",
    "    # Text input_ids are already padded to max_length=128 in dataset, but audio is raw\n",
    "    \n",
    "    batch = {}\n",
    "    \n",
    "    # Text fields (already tensors)\n",
    "    batch[\"input_ids\"] = torch.stack([f[\"input_ids\"] for f in features])\n",
    "    batch[\"attention_mask\"] = torch.stack([f[\"attention_mask\"] for f in features])\n",
    "    batch[\"labels\"] = torch.stack([f[\"labels\"] for f in features])\n",
    "    \n",
    "    # Audio fields\n",
    "    # Pad audio input_values to longest in batch\n",
    "    input_values = [f[\"input_values\"] for f in features]\n",
    "    # Simple pad sequence\n",
    "    max_len = max([v.shape[0] for v in input_values])\n",
    "    # Pad with 0.0 (silence)\n",
    "    padded_values = torch.zeros(len(input_values), max_len)\n",
    "    padded_mask = torch.zeros(len(input_values), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, v in enumerate(input_values): \n",
    "        l = v.shape[0]\n",
    "        padded_values[i, :l] = v\n",
    "        padded_mask[i, :l] = 1 # 1 for valid, 0 for pad\n",
    "        \n",
    "    batch[\"input_values\"] = padded_values\n",
    "    batch[\"audio_attention_mask\"] = padded_mask\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7deac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: stapesai/ssi-speech-emotion-recognition...\n",
      "Preparing labels...\n",
      "Labels: ['ANG', 'CAL', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD', 'SUR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Devv\\Machine Learning\\huggingface\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10000\n",
      "Val size: 1999\n",
      "Test size: 163\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}...\")\n",
    "ds = load_dataset(CONFIG[\"dataset_name\"])\n",
    "\n",
    "# Ensure audio is 16kHz\n",
    "ds = ds.cast_column(\"file_path\", Audio(sampling_rate=CONFIG[\"sample_rate\"]))\n",
    "\n",
    "# Prepare Labels\n",
    "print(\"Preparing labels...\")\n",
    "labels_list = sorted(list(set(ds[\"train\"][\"emotion\"])))\n",
    "label2id = {l: i for i, l in enumerate(labels_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "num_labels = len(labels_list)\n",
    "print(f\"Labels: {labels_list}\")\n",
    "\n",
    "# Initialize Processors\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(CONFIG[\"audio_model_name\"])\n",
    "text_tokenizer = RobertaTokenizer.from_pretrained(CONFIG[\"text_model_name\"])\n",
    "\n",
    "# Create Datasets\n",
    "# Use 'validation' split if available, else split train\n",
    "if \"validation\" in ds:\n",
    "    val_split = ds[\"validation\"]\n",
    "    train_split = ds[\"train\"]\n",
    "else:\n",
    "    # Fallback split\n",
    "    print(\"No validation split found. Splitting 'train'...\")\n",
    "    ds_split = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "    train_split = ds_split[\"train\"]\n",
    "    val_split = ds_split[\"test\"]\n",
    "    \n",
    "test_split = ds[\"test\"] if \"test\" in ds else val_split\n",
    "\n",
    "train_ds = MultimodalEmotionDataset(train_split, label2id, audio_processor, text_tokenizer, augment=True)\n",
    "val_ds = MultimodalEmotionDataset(val_split, label2id, audio_processor, text_tokenizer)\n",
    "test_ds = MultimodalEmotionDataset(test_split, label2id, audio_processor, text_tokenizer)\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size: {len(val_ds)}\")\n",
    "print(f\"Test size: {len(test_ds)}\")\n",
    "\n",
    "# Initialize Model\n",
    "model = MultimodalEmotionModel(num_labels).to(device)\n",
    "if CONFIG.get(\"freeze_feature_encoder\", True):\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "# Training Arguments\n",
    "tb_log_dir = os.path.join(CONFIG[\"output_dir\"], \"runs\")\n",
    "run_name = f\"multimodal-emotion-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    run_name=run_name,\n",
    "    logging_dir=tb_log_dir,\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=CONFIG[\"epochs\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    gradient_checkpointing=False,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False, # Essential for passing non-standard args (audio_attention_mask)\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "final_model_path = os.path.join(CONFIG[\"output_dir\"], \"model.pt\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "audio_processor.save_pretrained(CONFIG[\"output_dir\"])\n",
    "text_tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
    "\n",
    "with open(os.path.join(CONFIG[\"output_dir\"], \"labels.json\"), \"w\") as f:\n",
    "    json.dump(label2id, f, indent=2)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== TEST SET EVALUATION ===\")\n",
    "preds = trainer.predict(test_ds)\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "y_true = preds.label_ids\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=labels_list,\n",
    "        zero_division=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", xticklabels=labels_list, yticklabels=labels_list\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"confusion_matrix.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
